# Лабораторная работа №2

## Исполнители

Обучающиеся группы 449м: Евсеенкова Кристина Денисовна, Старков Силантий Денисович, Ташкова Анна Юрьевна

## Цель работы

Научиться работать с предобученными моделями и на основе предобученных эмбеддингов строить новые модели.

## Оглавление

*   [1. Теоретическая база](#1-теоретическая-база)
    *   [1.1 Архитектура трансформеров](#11-архитектура-трансформеров)
    *   [1.2 Эмбеддинги](#12-эмбеддинги)
*   [2. Эмбеддинг ALBERT](#2-эмбеддинг-albert)
    *   [2.1 Что такое ALBERT?](#21-что-такое-albert)
    *   [2.2 Основные особенности ALBERT](#22-основные-особенности-albert)
    *   [2.3 Применение ALBERT](#23-применение-albert)
    *   [2.4 Преимущества и недостатки](#24-преимущества-и-недостатки)
*   [3. Результаты работы и тестирования системы](#3-результаты-работы-и-тестирования-системы)
*   [4. Выводы по работе](#4-выводы-по-работе)
*   [Список использованных источников](#список-использованных-источников)

# ALBERT (A Lite BERT)

ALBERT — это улучшенная версия модели BERT, предназначенная для повышения эффективности и снижения вычислительных требований, сохраняя при этом высокую производительность на задачах обработки естественного языка.

## 1. Теоретическая база

### 1.1 Архитектура трансформеров

Трансформеры — это архитектура нейронных сетей, предложенная в 2017 году в статье *"Attention is All You Need"*. Они предназначены для обработки последовательностей данных, таких как текст, и основываются на механизме внимания, который позволяет моделям учитывать контекст при обработке входных данных. Основные компоненты трансформеров:

- **Механизм внимания**: Позволяет модели фокусироваться на различных частях входной последовательности.
- **Многоуровневая структура**: Состоит из нескольких слоев, каждый из которых включает механизмы внимания и полносвязные слои.
- **Позиционные эмбеддинги**: Добавляют информацию о позиции каждого слова в последовательности.
- **Нормализация и остаточные соединения**: Используются для улучшения сходимости и предотвращения исчезновения градиента.

### 1.2 Эмбеддинги

Эмбеддинги — это представления слов или токенов в виде векторов фиксированной размерности. Они позволяют моделям захватывать семантические и синтаксические отношения между словами. Основные аспекты теории эмбеддингов:

- **Обучение эмбеддингов**: Эмбеддинги могут быть обучены с нуля или предобучены на больших объемах текстовых данных.
- **Контекстуальные эмбеддинги**: Создают разные представления для одного и того же слова в зависимости от его контекста.
- **Применение эмбеддингов**: Используются в задачах классификации текста, анализа настроений, ответа на вопросы и генерации текста.

## 2. Эмбеддинг ALBERT

### 2.1 Что такое ALBERT?

ALBERT (A Lite BERT) — это улучшенная версия модели BERT, разработанная для повышения эффективности и уменьшения требований к вычислительным ресурсам.

### 2.2 Основные особенности ALBERT

1. **Разделение параметров**: Позволяет значительно сократить количество параметров в модели.
2. **Многоуровневая архитектура**: Сохраняет способность к пониманию сложных языковых структур.
3. **Обучение на больших объемах данных**: Позволяет извлекать более глубокие семантические и синтаксические связи.
4. **Контекстуальные эмбеддинги**: Создает разные представления для одного и того же слова в зависимости от контекста.

### 2.3 Применение ALBERT

ALBERT можно использовать для множества задач в области обработки естественного языка, включая:

- **Классификация текста**
- **Ответ на вопросы**
- **Анализ настроений**
- **Генерация текста**

### 2.4 Преимущества и недостатки

**Преимущества**:
- Более легковесная модель по сравнению с BERT.
- Высокая производительность на различных задачах.

**Недостатки**:
- Может потребовать больше времени на обучение.
- В некоторых случаях может не достигать такой же точности, как более крупные модели.

## 3. Результаты работы и тестирования системы 

В результате выполнения лабораторной работы была реализована нейронная сеть AlexNet с оптимизатором AdaSmoothDelta. Архитектура сети была реализована с использованием библиотеки Torch. Обучение модели проводилось с использованием двух различных оптимизаторов — Adam и AdaSmoothDelta. Результаты обучения нейронной сети с этими оптимизаторами представлены на графиках, показывающих изменения потерь и точности на обучающей и тестовой выборках в процессе обучения.


<center>Рисунок 2 - Результат обучения нейронной сети с оптимизатором Adam</center>

<center>Рисунок 3 - Результат обучения нейронной сети с оптимизатором AdaSmoothDelta</center>

График точности приведены на рисунке 4.

<center>Рисунок 4 -  График точности </center>

## Выводы по работе

В результате выполнения лабораторной работы была реализована нейронная сеть AlexNet с оптимизатором AdaSmoothDelta. Архитектура была реализована с использованием Torch.

Точность модели с оптимизатором Adam составила 0.85%.

Точность модели с оптимизатором AdaSmoothDelta составила ?%.

Таким образом, AdaSmoothDelta продемонстрировал хорошую стабильность на протяжении обучения, несмотря на то, что точности по обеим моделям были схожи. Это может свидетельствовать о том, что оптимизатор AdaSmoothDelta способен более эффективно адаптироваться к различным этапам обучения.



## Список использованных источников

1.  Хайкин, С. Нейронные сети : полный курс / С. Хайкин. – 2-е изд., испр. – Москва : Вильямс, 2006. – 1104 с.
2.  Гудфеллоу, А. Глубокое обучение / А. Гудфеллоу, И. Бенджио, А. Курвилль ; пер. с англ. – Москва : ДМК Пресс, 2018. – 652 с.
3.  Рассел, С. Искусственный интеллект: современный подход / С. Рассел, П. Норвиг ; пер. с англ. – 2-е изд. – Москва : Вильямс, 2006. – 1408 с.
4.  Иванов, Д. А. Анализ и прогнозирование временных рядов с использованием нейронных сетей / Д. А. Иванов // Информационные технологии. – 2018. – № 4. – С. 25-29.
5.  Петров, В. В. Разработка экспертной системы для диагностики заболеваний / В. В. Петров, С. И. Сидоров // Искусственный интеллект и принятие решений. – 2020. – № 2. – С. 15-22.
6.  Сергеев, А. С. Применение генетических алгоритмов для оптимизации параметров нейронных сетей / А. С. Сергеев // Проблемы управления. – 2019. – № 5. – С. 30-35.
