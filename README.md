# Лабораторная работа №2

## Исполнители

Обучающиеся группы 449м: Евсеенкова Кристина Денисовна, Старков Силантий Денисович, Ташкова Анна Юрьевна

## Цель работы

Научиться работать с предобученными моделями и на основе предобученных эмбеддингов строить новые модели.

## Оглавление

*   [1. Теоретическая база](#1-теоретическая-база)
    *   [1.1 Архитектура трансформеров](#11-архитектура-трансформеров)
    *   [1.2 Эмбеддинги](#12-эмбеддинги)
*   [2. Эмбеддинг ALBERT](#2-эмбеддинг-albert)
    *   [2.1 Что такое ALBERT?](#21-что-такое-albert)
    *   [2.2 Основные особенности ALBERT](#22-основные-особенности-albert)
    *   [2.3 Применение ALBERT](#23-применение-albert)
    *   [2.4 Преимущества и недостатки](#24-преимущества-и-недостатки)
*   [3. Результаты работы и тестирования системы](#3-результаты-работы-и-тестирования-системы)
*   [4. Выводы по работе](#4-выводы-по-работе)
*   [Список использованных источников](#список-использованных-источников)

# ALBERT (A Lite BERT)

ALBERT — это улучшенная версия модели BERT (Bidirectional Encoder Representations from Transformers), разработанная для повышения эффективности и снижения вычислительных требований, сохраняя при этом высокую производительность на задачах обработки естественного языка. Эта модель была представлена в 2019 году и быстро завоевала популярность благодаря своим инновационным подходам к архитектуре и обучению.

## 1. Теоретическая база

### 1.1 Архитектура трансформеров

Трансформеры — это архитектура нейронных сетей, предложенная в 2017 году в статье *"Attention is All You Need"*. Они предназначены для обработки последовательностей данных, таких как текст, и основываются на механизме внимания, который позволяет моделям учитывать контекст при обработке входных данных. Основные компоненты трансформеров:

- **Механизм внимания**: Позволяет модели фокусироваться на различных частях входной последовательности, взвешивая важность каждого слова в контексте других.
  
- **Многоуровневая структура**: Состоит из нескольких слоев, каждый из которых включает механизмы внимания и полносвязные слои.

- **Позиционные эмбеддинги**: Добавляют информацию о позиции каждого слова в последовательности.

- **Нормализация и остаточные соединения**: Используются для улучшения сходимости и предотвращения исчезновения градиента.
![-y7ztlhyq5_817b89qhjjikwopk](https://github.com/user-attachments/assets/10520e09-880c-451e-93f7-aa6dc5b59e37)

*Рисунок 1: Архитектура трансформеров. Основные компоненты трансформера: механизм внимания, многослойная структура и позиционные эмбеддинги.*

### 1.2 Эмбеддинги

Эмбеддинги — это представления слов или токенов в виде векторов фиксированной размерности. Они позволяют моделям захватывать семантические и синтаксические отношения между словами. Основные аспекты теории эмбеддингов:

- **Обучение эмбеддингов**: Эмбеддинги могут быть обучены с нуля или предобучены на больших объемах текстовых данных.

- **Контекстуальные эмбеддинги**: Создают разные представления для одного и того же слова в зависимости от его контекста.

- **Применение эмбеддингов**: Используются в задачах классификации текста, анализа настроений, ответа на вопросы и генерации текста.

![image](https://github.com/user-attachments/assets/ee61de78-4a0c-4448-9e9d-d2841e8f9fc0)

*Рисунок 2: Эмбеддинги. Пример векторного представления слов, показывающий семантические отношения между ними.*

## 2. Эмбеддинг ALBERT

### 2.1 Что такое ALBERT?

ALBERT (A Lite BERT) — это улучшенная версия модели BERT, разработанная для повышения эффективности и уменьшения требований к вычислительным ресурсам. Основная цель ALBERT заключается в том, чтобы сохранить высокую производительность на задачах обработки естественного языка при значительно меньших вычислительных затратах.

### 2.2 Основные особенности ALBERT

1. **Разделение параметров**: Позволяет значительно сократить количество параметров в модели.

2. **Многоуровневая архитектура**: Сохраняет способность к пониманию сложных языковых структур.

3. **Обучение на больших объемах данных**: ALBERT использует предобучение на больших корпусах текстов.

4. **Контекстуальные эмбеддинги**: Создает разные представления для одного и того же слова в зависимости от контекста.
   
![The-architecture-of-the-ALBERT-model-in-our-task](https://github.com/user-attachments/assets/f48019f5-bd74-416c-87ee-9c1ede627f2a)

*Рисунок 3: ALBERT. Иллюстрация, демонстрирующая архитектуру ALBERT и его ключевые особенности.*

### 2.3 Применение ALBERT

ALBERT можно использовать для множества задач в области обработки естественного языка:

- **Классификация текста**: Эффективно классифицирует текстовые документы по темам и жанрам.

- **Ответ на вопросы**: Обрабатывает запросы и извлекает информацию из текстов.

- **Анализ настроений**: Определяет эмоциональную окраску текста.

- **Генерация текста**: Создает связный и осмысленный текст.

### 2.4 Преимущества и недостатки

**Преимущества**:
- **Более легковесная модель**: ALBERT значительно легче, чем BERT.
- **Высокая производительность**: Конкурентоспособные результаты на различных задачах NLP.
- **Снижение времени обучения**: Требует меньше времени для обучения.

**Недостатки**:
- **Время обучения**: Может потребовать больше времени на обучение по сравнению с другими моделями.
- **Точность**: Может не достигать такой же точности, как более крупные модели на сложных задачах.

## 3. Результаты работы и тестирования системы 

В результате выполнения лабораторной работы была реализована нейронная сеть AlexNet с оптимизатором AdaSmoothDelta. Архитектура сети была реализована с использованием библиотеки Torch. Обучение модели проводилось с использованием двух различных оптимизаторов — Adam и AdaSmoothDelta. Результаты обучения нейронной сети с этими оптимизаторами представлены на графиках, показывающих изменения потерь и точности на обучающей и тестовой выборках в процессе обучения.


<center>Рисунок 2 - Результат обучения нейронной сети с оптимизатором Adam</center>

<center>Рисунок 3 - Результат обучения нейронной сети с оптимизатором AdaSmoothDelta</center>

График точности приведены на рисунке 4.

<center>Рисунок 4 -  График точности </center>

## Выводы по работе

В результате выполнения лабораторной работы была реализована нейронная сеть AlexNet с оптимизатором AdaSmoothDelta. Архитектура была реализована с использованием Torch.

Точность модели с оптимизатором Adam составила 0.85%.

Точность модели с оптимизатором AdaSmoothDelta составила ?%.

Таким образом, AdaSmoothDelta продемонстрировал хорошую стабильность на протяжении обучения, несмотря на то, что точности по обеим моделям были схожи. Это может свидетельствовать о том, что оптимизатор AdaSmoothDelta способен более эффективно адаптироваться к различным этапам обучения.



## Список использованных источников

1.  Хайкин, С. Нейронные сети : полный курс / С. Хайкин. – 2-е изд., испр. – Москва : Вильямс, 2006. – 1104 с.
2.  Гудфеллоу, А. Глубокое обучение / А. Гудфеллоу, И. Бенджио, А. Курвилль ; пер. с англ. – Москва : ДМК Пресс, 2018. – 652 с.
3.  Рассел, С. Искусственный интеллект: современный подход / С. Рассел, П. Норвиг ; пер. с англ. – 2-е изд. – Москва : Вильямс, 2006. – 1408 с.
4.  Иванов, Д. А. Анализ и прогнозирование временных рядов с использованием нейронных сетей / Д. А. Иванов // Информационные технологии. – 2018. – № 4. – С. 25-29.
5.  Петров, В. В. Разработка экспертной системы для диагностики заболеваний / В. В. Петров, С. И. Сидоров // Искусственный интеллект и принятие решений. – 2020. – № 2. – С. 15-22.
6.  Сергеев, А. С. Применение генетических алгоритмов для оптимизации параметров нейронных сетей / А. С. Сергеев // Проблемы управления. – 2019. – № 5. – С. 30-35.
